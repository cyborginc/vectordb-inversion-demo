{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b404d7",
   "metadata": {},
   "source": [
    "# ChromaDB + Vec2Text Vulnerability Demonstration\n",
    "\n",
    "This notebook shows how sensitive data stored in ChromaDB can be reconstructed using vec2text\n",
    " \n",
    "### Attack Chain:\n",
    "1. Sensitive texts → OpenAI embeddings → ChromaDB storage\n",
    "2. Extract embeddings from SQLite database\n",
    "3. Use vec2text to reconstruct original sensitive text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vec2text\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "import sqlite3\n",
    "import struct\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# OpenAI setup\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "client = OpenAI()\n",
    "\n",
    "# Load vec2text corrector for inversion\n",
    "print(f\"Loading vec2text corrector for OpenAI {embedding_model}...\")\n",
    "corrector = vec2text.load_pretrained_corrector(\"text-embedding-ada-002\")\n",
    "\n",
    "# ChromaDB setup\n",
    "persist_directory = \"./chromadb_vuln_demo\"\n",
    "chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "\n",
    "# Create or reset collection\n",
    "try:\n",
    "    chroma_client.delete_collection(\"sensitive_docs\")\n",
    "except:\n",
    "    pass\n",
    "collection = chroma_client.create_collection(\"sensitive_docs\")\n",
    "\n",
    "print(f\"ChromaDB storage location: {persist_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sensitive Test Corpus (same as vec2text demo)\n",
    "\n",
    "sensitive_documents = [\n",
    "    # Financial/Identity Information\n",
    "    \"My social security number is 818-88-8118\",\n",
    "    \"Password for admin account is SecurePass123!\",\n",
    "    \"Credit card number 4532-1234-5678-9012 expires next month\",\n",
    "    \"API key for production: sk-proj-1234567890abcdef\",\n",
    "    \"Bank account routing number 021000021 checking 1234567890\",\n",
    "    \n",
    "    # Medical Information\n",
    "    \"Patient diagnosed with Type 2 diabetes, prescribed metformin\",\n",
    "    \"Blood test results show cholesterol level of 277 mg/dL\",\n",
    "    \n",
    "    # Personal Information\n",
    "    \"John Doe lives at 1 World Trade Center, New York, NY 10007\",\n",
    "    \"Phone number +1(212) 653-0688; email info@cyborg.co\",\n",
    "]\n",
    "\n",
    "print(f\"Testing with {len(sensitive_documents)} sensitive documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generate OpenAI Embeddings and Store in ChromaDB\n",
    "\n",
    "def get_embeddings_openai(text_list):\n",
    "    \"\"\"Get embeddings from OpenAI API\"\"\"\n",
    "    print(f\"Generating embeddings for {len(text_list)} texts...\")\n",
    "    response = client.embeddings.create(\n",
    "        input=text_list,\n",
    "        model=embedding_model,\n",
    "        encoding_format=\"float\",\n",
    "    )\n",
    "    return [e.embedding for e in response.data]\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = get_embeddings_openai(sensitive_documents)\n",
    "print(f\"Generated {len(embeddings)} embeddings of dimension {len(embeddings[0])}\")\n",
    "\n",
    "# Store in ChromaDB\n",
    "collection.add(\n",
    "    documents=sensitive_documents,\n",
    "    embeddings=embeddings,\n",
    "    ids=[f\"sensitive_doc_{i}\" for i in range(len(sensitive_documents))],\n",
    "    metadatas=[{\"type\": \"sensitive\", \"doc_num\": i} for i in range(len(sensitive_documents))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f907f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extract Embeddings from ChromaDB SQLite Database\n",
    "\n",
    "print(\"\\n=== EXTRACTING EMBEDDINGS FROM CHROMADB DATABASE ===\")\n",
    "\n",
    "# Connect to ChromaDB's SQLite database\n",
    "db_path = os.path.join(persist_directory, \"chroma.sqlite3\")\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(f\"Connected to SQLite database: {db_path}\")\n",
    "\n",
    "# Get embedding data from database\n",
    "cursor.execute(\"SELECT * FROM embeddings_queue ORDER BY seq_id\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "print(f\"Found {len(rows)} embedding records in database\")\n",
    "\n",
    "# Extract embeddings and documents\n",
    "extracted_embeddings = []\n",
    "extracted_documents = []\n",
    "\n",
    "for i, row in enumerate(rows):\n",
    "    # Parse the row structure\n",
    "    doc_id = row[4]  # Document ID\n",
    "    embedding_blob = row[5]  # Binary embedding data\n",
    "    metadata_json = row[7]  # JSON metadata\n",
    "    \n",
    "    # Parse metadata to get original document\n",
    "    metadata = json.loads(metadata_json)\n",
    "    original_doc = metadata.get('chroma:document', 'Unknown')\n",
    "    \n",
    "    # Parse embedding blob (FLOAT32 binary data)\n",
    "    num_floats = len(embedding_blob) // 4\n",
    "    embedding_values = struct.unpack(f'{num_floats}f', embedding_blob)\n",
    "    \n",
    "    extracted_embeddings.append(list(embedding_values))\n",
    "    extracted_documents.append(original_doc)\n",
    "    \n",
    "    print(f\"Extracted embedding {i+1}: {doc_id}\")\n",
    "\n",
    "print(f\"\\nSuccessfully extracted {len(extracted_embeddings)} embeddings from database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35933773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Convert to PyTorch & Invert Each Embedding\n",
    "\n",
    "print(\"\\n=== RUNNING VEC2TEXT INVERSION ON EXTRACTED EMBEDDINGS ===\")\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "embeddings_tensor = torch.tensor(extracted_embeddings, dtype=torch.float32)\n",
    "\n",
    "# Move to appropriate device\n",
    "if torch.backends.mps.is_available():\n",
    "    embeddings_tensor = embeddings_tensor.to('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    embeddings_tensor = embeddings_tensor.cuda()\n",
    "\n",
    "print(f\"Embeddings tensor shape: {embeddings_tensor.shape}\")\n",
    "print(f\"Device: {embeddings_tensor.device}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INVERSION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (original_doc, embedding) in enumerate(zip(extracted_documents, embeddings_tensor)):\n",
    "    print(f\"\\nDocument #{i+1}:\")\n",
    "    print(f\"\\nOriginal (from database): {original_doc}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Invert embedding\n",
    "    reconstructed_list = vec2text.invert_embeddings(\n",
    "        embeddings=embedding.unsqueeze(0),  # Add batch dimension\n",
    "        corrector=corrector,\n",
    "        num_steps=20,\n",
    "        sequence_beam_width=1,\n",
    "    )\n",
    "    reconstructed = reconstructed_list[0]\n",
    "    \n",
    "    inversion_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Reconstructed: {reconstructed}\")\n",
    "    \n",
    "    # Calculate similarity\n",
    "    orig_emb_cpu = embedding.cpu()\n",
    "    new_emb = get_embeddings_openai([reconstructed])[0]\n",
    "    new_emb_tensor = torch.tensor(new_emb)\n",
    "    similarity = torch.nn.functional.cosine_similarity(orig_emb_cpu, new_emb_tensor, dim=0).item()\n",
    "    \n",
    "    # Check exact match\n",
    "    exact_match = original_doc.lower().strip() == reconstructed.lower().strip()\n",
    "    \n",
    "    print(f\"Similarity: {similarity:.4f}\")\n",
    "    print(f\"Inversion time: {inversion_time:.2f}s\")\n",
    "    print(f\"Exact match: {exact_match}\")\n",
    "    \n",
    "    results.append({\n",
    "        'original': original_doc,\n",
    "        'reconstructed': reconstructed,\n",
    "        'similarity': similarity,\n",
    "        'time': inversion_time,\n",
    "        'exact_match': exact_match,\n",
    "    })\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Summary Analysis\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VULNERABILITY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_docs = len(results)\n",
    "exact_matches = sum(1 for r in results if r['exact_match'])\n",
    "high_similarity = sum(1 for r in results if r['similarity'] > 0.95)\n",
    "sensitive_recovered = sum(1 for r in results if r['sensitive_recovered'])\n",
    "\n",
    "print(f\"Total documents processed: {total_docs}\")\n",
    "print(f\"Exact reconstructions: {exact_matches} ({exact_matches/total_docs*100:.1f}%)\")\n",
    "print(f\"High similarity (>95%): {high_similarity} ({high_similarity/total_docs*100:.1f}%)\")\n",
    "\n",
    "avg_similarity = np.mean([r['similarity'] for r in results])\n",
    "avg_time = np.mean([r['time'] for r in results])\n",
    "\n",
    "print(f\"Average similarity: {avg_similarity:.4f}\")\n",
    "print(f\"Average inversion time: {avg_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nATTACK CHAIN COMPLETE:\")\n",
    "print(f\"   1. Stored sensitive data in ChromaDB\")\n",
    "print(f\"   2. Extracted embeddings from SQLite database\") \n",
    "print(f\"   3. Reconstructed {sensitive_recovered}/{total_docs} sensitive documents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

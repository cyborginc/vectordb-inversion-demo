{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d931f6",
   "metadata": {},
   "source": [
    "## Iterative Optimization for Embedding Inversion\n",
    "This notebook implements gradient descent in input space to invert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2bbc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolas/miniconda3/envs/py313/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Creating embeddings for 21 texts...\n",
      "Embedding shape: (21, 384)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 1. Setup and Create Diverse Test Corpus\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model components\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "sentence_model = SentenceTransformer(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "transformer_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Create diverse test corpus\n",
    "test_corpus = [\n",
    "    # News headlines\n",
    "    \"Scientists discover new species of deep-sea fish in Pacific Ocean\",\n",
    "    \"Stock market reaches all-time high amid economic recovery\",\n",
    "    \"Climate change accelerates Arctic ice melt, study finds\",\n",
    "    \n",
    "    # Technical descriptions\n",
    "    \"The algorithm uses dynamic programming to optimize runtime complexity\",\n",
    "    \"Machine learning model achieves 95% accuracy on test dataset\",\n",
    "    \"Quantum computers leverage superposition for parallel processing\",\n",
    "    \n",
    "    # Questions\n",
    "    \"What causes the northern lights to appear in the sky?\",\n",
    "    \"How does photosynthesis convert sunlight into energy?\",\n",
    "    \"Why do some materials conduct electricity better than others?\",\n",
    "    \n",
    "    # Instructions\n",
    "    \"Mix flour, eggs, and milk to create pancake batter\",\n",
    "    \"Press and hold the power button for 10 seconds to reset\",\n",
    "    \"Apply two coats of paint, allowing each to dry completely\",\n",
    "    \n",
    "    # Conversational\n",
    "    \"I really enjoyed the movie we watched last night\",\n",
    "    \"The weather has been unusually warm for this time of year\",\n",
    "    \"Let's meet at the coffee shop downtown at 3 PM\",\n",
    "    \n",
    "    # Literary/Quotes\n",
    "    \"To be or not to be, that is the question\",\n",
    "    \"The journey of a thousand miles begins with a single step\",\n",
    "    \"All that glitters is not gold\",\n",
    "    \n",
    "    # Facts\n",
    "    \"The human brain contains approximately 86 billion neurons\",\n",
    "    \"Water boils at 100 degrees Celsius at sea level\",\n",
    "    \"The speed of light in vacuum is 299,792,458 meters per second\"\n",
    "]\n",
    "\n",
    "# Create embeddings\n",
    "print(f\"Creating embeddings for {len(test_corpus)} texts...\")\n",
    "target_embeddings = sentence_model.encode(test_corpus)\n",
    "print(f\"Embedding shape: {target_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65abbb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. Implement Iterative Optimization Inversion\n",
    "\n",
    "# %%\n",
    "class EmbeddingInverter:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.embedding_dim = model.config.hidden_size\n",
    "        \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"Replicate the mean pooling used by sentence-transformers\"\"\"\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def forward_pass(self, input_embeds, attention_mask):\n",
    "        \"\"\"Forward pass through the model with continuous embeddings\"\"\"\n",
    "        outputs = self.model(inputs_embeds=input_embeds, attention_mask=attention_mask)\n",
    "        sentence_embeddings = self.mean_pooling(outputs, attention_mask)\n",
    "        # Normalize embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings\n",
    "    \n",
    "    def invert_embedding(self, target_embedding, num_tokens=10, num_iterations=1000, lr=0.01):\n",
    "        \"\"\"\n",
    "        Invert embedding using gradient descent on input embeddings\n",
    "        \"\"\"\n",
    "        # Convert target to tensor\n",
    "        target = torch.tensor(target_embedding, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        target = F.normalize(target, p=2, dim=1)\n",
    "        \n",
    "        # Initialize random embeddings for tokens\n",
    "        # Start with embeddings close to average token embedding\n",
    "        with torch.no_grad():\n",
    "            # Get embeddings for common words as initialization\n",
    "            init_tokens = self.tokenizer.encode(\"the of and to in is\", add_special_tokens=False)\n",
    "            init_embeds = self.model.embeddings.word_embeddings(torch.tensor(init_tokens).to(self.device))\n",
    "            avg_embed = init_embeds.mean(dim=0)\n",
    "        \n",
    "        # Learnable embeddings (including [CLS] and [SEP])\n",
    "        input_embeds = avg_embed.unsqueeze(0).repeat(1, num_tokens + 2, 1).clone().detach()\n",
    "        input_embeds.requires_grad = True\n",
    "        \n",
    "        # Set [CLS] and [SEP] tokens\n",
    "        cls_id = self.tokenizer.cls_token_id\n",
    "        sep_id = self.tokenizer.sep_token_id\n",
    "        input_embeds.data[0, 0] = self.model.embeddings.word_embeddings(torch.tensor([cls_id]).to(self.device))\n",
    "        input_embeds.data[0, -1] = self.model.embeddings.word_embeddings(torch.tensor([sep_id]).to(self.device))\n",
    "        \n",
    "        # Attention mask (all ones)\n",
    "        attention_mask = torch.ones(1, num_tokens + 2).to(self.device)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = optim.Adam([input_embeds], lr=lr)\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        # Optimization loop\n",
    "        for iteration in range(num_iterations):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            predicted = self.forward_pass(input_embeds, attention_mask)\n",
    "            \n",
    "            # Compute loss (negative cosine similarity)\n",
    "            loss = 1 - F.cosine_similarity(predicted, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update only the middle tokens (not CLS/SEP)\n",
    "            with torch.no_grad():\n",
    "                input_embeds.grad[0, 0] = 0\n",
    "                input_embeds.grad[0, -1] = 0\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            if iteration % 200 == 0:\n",
    "                print(f\"Iteration {iteration}, Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        # Find nearest tokens for each embedding\n",
    "        final_embeds = input_embeds.detach()\n",
    "        reconstructed_tokens = self.find_nearest_tokens(final_embeds[0, 1:-1])  # Exclude CLS/SEP\n",
    "        \n",
    "        return reconstructed_tokens, losses, final_embeds\n",
    "    \n",
    "    def find_nearest_tokens(self, embeddings):\n",
    "        \"\"\"Find nearest tokens in vocabulary for continuous embeddings\"\"\"\n",
    "        # Get all token embeddings\n",
    "        vocab_size = self.tokenizer.vocab_size\n",
    "        all_token_ids = torch.arange(vocab_size).to(self.device)\n",
    "        all_token_embeds = self.model.embeddings.word_embeddings(all_token_ids)\n",
    "        \n",
    "        reconstructed_tokens = []\n",
    "        \n",
    "        for embed in embeddings:\n",
    "            # Compute distances to all tokens\n",
    "            distances = torch.cdist(embed.unsqueeze(0), all_token_embeds).squeeze()\n",
    "            \n",
    "            # Find top 5 nearest tokens\n",
    "            nearest_ids = torch.argsort(distances)[:5]\n",
    "            nearest_tokens = [self.tokenizer.decode([token_id]) for token_id in nearest_ids]\n",
    "            nearest_distances = [distances[token_id].item() for token_id in nearest_ids]\n",
    "            \n",
    "            reconstructed_tokens.append({\n",
    "                'best_token': nearest_tokens[0],\n",
    "                'alternatives': list(zip(nearest_tokens[1:], nearest_distances[1:]))\n",
    "            })\n",
    "        \n",
    "        return reconstructed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb63128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Original text: Scientists discover new species of deep-sea fish in Pacific Ocean\n",
      "============================================================\n",
      "\n",
      "Trying with 5 tokens:\n",
      "Iteration 0, Loss: 1.054420\n",
      "Iteration 200, Loss: 0.023472\n",
      "Iteration 400, Loss: 0.007763\n",
      "Iteration 600, Loss: 0.002564\n",
      "Iteration 800, Loss: 0.002276\n",
      "Reconstructed: [unused80] [unused80] a a )\n",
      "Similarity: -0.0022\n",
      "Token alternatives:\n",
      "  Position 0: [unused80] | alternatives: -(2.49), in(2.49)\n",
      "  Position 1: [unused80] | alternatives: is(2.48), on(2.49)\n",
      "  Position 2: a | alternatives: an(2.49), with(2.50)\n",
      "  Position 3: a | alternatives: the(2.52), an(2.52)\n",
      "  Position 4: ) | alternatives: and(2.51), ,(2.51)\n",
      "\n",
      "Trying with 10 tokens:\n",
      "Iteration 0, Loss: 1.079188\n",
      "Iteration 200, Loss: 0.000036\n",
      "Iteration 400, Loss: 0.000000\n",
      "Iteration 600, Loss: 0.000000\n",
      "Iteration 800, Loss: 0.000000\n",
      "Reconstructed: in are the a - the [unused80] in [SEP] in\n",
      "Similarity: 0.0523\n",
      "Token alternatives:\n",
      "  Position 0: in | alternatives: of(2.11), with(2.15)\n",
      "  Position 1: are | alternatives: in(2.00), is(2.00)\n",
      "  Position 2: the | alternatives: in(1.70), a(1.71)\n",
      "  Position 3: a | alternatives: the(1.84), are(1.84)\n",
      "  Position 4: - | alternatives: the(1.79), a(1.82)\n",
      "\n",
      "Trying with 15 tokens:\n",
      "Iteration 0, Loss: 1.108915\n",
      "Iteration 200, Loss: 0.000045\n",
      "Iteration 400, Loss: 0.000000\n",
      "Iteration 600, Loss: -0.000000\n",
      "Iteration 800, Loss: 0.000000\n",
      "Reconstructed: in in in in in [unused80] in to in is in in in in in\n",
      "Similarity: -0.0021\n",
      "Token alternatives:\n",
      "  Position 0: in | alternatives: of(1.77), to(1.82)\n",
      "  Position 1: in | alternatives: of(1.61), [unused80](1.66)\n",
      "  Position 2: in | alternatives: of(1.67), to(1.72)\n",
      "  Position 3: in | alternatives: of(1.60), is(1.61)\n",
      "  Position 4: in | alternatives: of(1.80), to(1.81)\n",
      "\n",
      "============================================================\n",
      "Original text: Stock market reaches all-time high amid economic recovery\n",
      "============================================================\n",
      "\n",
      "Trying with 5 tokens:\n",
      "Iteration 0, Loss: 0.939964\n",
      "Iteration 200, Loss: 0.000658\n",
      "Iteration 400, Loss: 0.000026\n",
      "Iteration 600, Loss: 0.000001\n",
      "Iteration 800, Loss: 0.000000\n",
      "Reconstructed: - ##↦ to of is\n",
      "Similarity: 0.0122\n",
      "Token alternatives:\n",
      "  Position 0: - | alternatives: in(1.93), a(1.96)\n",
      "  Position 1: ##↦ | alternatives: [unused69](1.91), [unused328](1.91)\n",
      "  Position 2: to | alternatives: a(1.72), in(1.73)\n",
      "  Position 3: of | alternatives: is(1.77), a(1.77)\n",
      "  Position 4: is | alternatives: are(1.95), 宇(1.96)\n",
      "\n",
      "Trying with 10 tokens:\n",
      "Iteration 0, Loss: 0.958197\n",
      "Iteration 200, Loss: 0.000336\n",
      "Iteration 400, Loss: 0.000011\n",
      "Iteration 600, Loss: 0.000001\n",
      "Iteration 800, Loss: 0.000000\n",
      "Reconstructed: of for in of a the the with ##₅ and\n",
      "Similarity: -0.0032\n",
      "Token alternatives:\n",
      "  Position 0: of | alternatives: for(1.86), -(1.87)\n",
      "  Position 1: for | alternatives: in(1.60), a(1.61)\n",
      "  Position 2: in | alternatives: of(1.76), -(1.76)\n",
      "  Position 3: of | alternatives: is(1.68), ₁(1.69)\n",
      "  Position 4: a | alternatives: an(1.80), the(1.81)\n",
      "\n",
      "Trying with 15 tokens:\n",
      "Iteration 0, Loss: 0.969398\n",
      "Iteration 200, Loss: 0.000087\n",
      "Iteration 400, Loss: 0.000001\n",
      "Iteration 600, Loss: 0.000000\n",
      "Iteration 800, Loss: 0.000000\n",
      "Reconstructed: ニ to in in a in of of of are and are is [unused80] the\n",
      "Similarity: 0.0348\n",
      "Token alternatives:\n",
      "  Position 0: ニ | alternatives: \"(1.74), with(1.74)\n",
      "  Position 1: to | alternatives: of(1.72), is(1.74)\n",
      "  Position 2: in | alternatives: for(1.72), and(1.73)\n",
      "  Position 3: in | alternatives: from(1.63), to(1.64)\n",
      "  Position 4: a | alternatives: to(1.54), in(1.54)\n",
      "\n",
      "============================================================\n",
      "Original text: Climate change accelerates Arctic ice melt, study finds\n",
      "============================================================\n",
      "\n",
      "Trying with 5 tokens:\n",
      "Iteration 0, Loss: 0.961982\n",
      "Iteration 200, Loss: 0.000540\n",
      "Iteration 400, Loss: 0.000019\n",
      "Iteration 600, Loss: 0.000282\n",
      "Iteration 800, Loss: 0.000162\n",
      "Reconstructed: in is [unused80] [unused80] are\n",
      "Similarity: -0.0785\n",
      "Token alternatives:\n",
      "  Position 0: in | alternatives: for(1.93), to(1.95)\n",
      "  Position 1: is | alternatives: a(1.98), was(1.99)\n",
      "  Position 2: [unused80] | alternatives: is(2.11), in(2.12)\n",
      "  Position 3: [unused80] | alternatives: ,(2.44), an(2.44)\n",
      "  Position 4: are | alternatives: in(2.06), [unused80](2.09)\n",
      "\n",
      "Trying with 10 tokens:\n",
      "Iteration 0, Loss: 0.977380\n",
      "Iteration 200, Loss: 0.000057\n",
      "Iteration 400, Loss: 0.000000\n",
      "Iteration 600, Loss: 0.000009\n",
      "Iteration 800, Loss: -0.000000\n",
      "Reconstructed: it in in in [unused80] in is [unused80] is [unused80]\n",
      "Similarity: -0.0327\n",
      "Token alternatives:\n",
      "  Position 0: it | alternatives: is(2.01), does(2.01)\n",
      "  Position 1: in | alternatives: and(1.59), [unused794](1.63)\n",
      "  Position 2: in | alternatives: and(1.60), the(1.62)\n",
      "  Position 3: in | alternatives: that(1.71), to(1.73)\n",
      "  Position 4: [unused80] | alternatives: in(2.08), to(2.11)\n",
      "\n",
      "Trying with 15 tokens:\n",
      "Iteration 0, Loss: 0.979378\n",
      "Iteration 200, Loss: 0.000072\n",
      "Iteration 400, Loss: 0.000001\n",
      "Iteration 600, Loss: 0.000001\n",
      "Iteration 800, Loss: 0.000000\n",
      "Reconstructed: is 火 in in in and have [unused80] [unused80] [unused80] and in is and arctic\n",
      "Similarity: 0.1976\n",
      "Token alternatives:\n",
      "  Position 0: is | alternatives: are(2.02), have(2.04)\n",
      "  Position 1: 火 | alternatives: to(2.16), h₂o(2.17)\n",
      "  Position 2: in | alternatives: and(1.67), [unused80](1.72)\n",
      "  Position 3: in | alternatives: and(1.70), that(1.72)\n",
      "  Position 4: in | alternatives: that(1.67), and(1.67)\n",
      "\n",
      "============================================================\n",
      "Original text: The algorithm uses dynamic programming to optimize runtime complexity\n",
      "============================================================\n",
      "\n",
      "Trying with 5 tokens:\n",
      "Iteration 0, Loss: 0.975926\n",
      "Iteration 200, Loss: 0.001282\n",
      "Iteration 400, Loss: 0.000182\n",
      "Iteration 600, Loss: 0.000132\n",
      "Iteration 800, Loss: 0.000006\n",
      "Reconstructed: in is ' 糹 of\n",
      "Similarity: 0.0751\n",
      "Token alternatives:\n",
      "  Position 0: in | alternatives: a(1.88), of(1.89)\n",
      "  Position 1: is | alternatives: in(1.94), an(1.95)\n",
      "  Position 2: ' | alternatives: [unused80](1.98), of(1.98)\n",
      "  Position 3: 糹 | alternatives: ##မ(1.80), ₒ(1.80)\n",
      "  Position 4: of | alternatives: is(1.84), a(1.85)\n",
      "\n",
      "Trying with 10 tokens:\n",
      "Iteration 0, Loss: 0.959569\n",
      "Iteration 200, Loss: 0.000354\n",
      "Iteration 400, Loss: 0.000006\n",
      "Iteration 600, Loss: 0.000000\n",
      "Iteration 800, Loss: -0.000000\n",
      "Reconstructed: is a that is was of of of of of\n",
      "Similarity: 0.0770\n",
      "Token alternatives:\n",
      "  Position 0: is | alternatives: an(1.69), a(1.70)\n",
      "  Position 1: a | alternatives: is(1.86), an(1.87)\n",
      "  Position 2: that | alternatives: is(2.00), was(2.01)\n",
      "  Position 3: is | alternatives: of(1.80), was(1.81)\n",
      "  Position 4: was | alternatives: is(1.86), [unused80](1.88)\n",
      "\n",
      "Trying with 15 tokens:\n",
      "Iteration 0, Loss: 0.953626\n",
      "Iteration 200, Loss: 0.000193\n",
      "Iteration 400, Loss: 0.000003\n",
      "Iteration 600, Loss: 0.000033\n",
      "Iteration 800, Loss: 0.000026\n",
      "Reconstructed: the was is is of of of to of of of of of of of\n",
      "Similarity: 0.1140\n",
      "Token alternatives:\n",
      "  Position 0: the | alternatives: is(1.77), in(1.78)\n",
      "  Position 1: was | alternatives: is(1.98), '(1.99)\n",
      "  Position 2: is | alternatives: to(2.06), was(2.08)\n",
      "  Position 3: is | alternatives: an(1.94), of(1.95)\n",
      "  Position 4: of | alternatives: is(1.72), was(1.74)\n",
      "\n",
      "============================================================\n",
      "Original text: Machine learning model achieves 95% accuracy on test dataset\n",
      "============================================================\n",
      "\n",
      "Trying with 5 tokens:\n",
      "Iteration 0, Loss: 0.995831\n",
      "Iteration 200, Loss: 0.001596\n",
      "Iteration 400, Loss: 0.000030\n",
      "Iteration 600, Loss: 0.000002\n",
      "Iteration 800, Loss: 0.000000\n",
      "Reconstructed: of ₍ a a ##ʊ\n",
      "Similarity: 0.0871\n",
      "Token alternatives:\n",
      "  Position 0: of | alternatives: in(2.21), dragoons(2.21)\n",
      "  Position 1: ₍ | alternatives: ##ᵒ(2.12), 原(2.13)\n",
      "  Position 2: a | alternatives: on(2.02), an(2.03)\n",
      "  Position 3: a | alternatives: ノ(2.21), [MASK](2.21)\n",
      "  Position 4: ##ʊ | alternatives: ज(2.45), מ(2.45)\n",
      "\n",
      "Trying with 10 tokens:\n",
      "Iteration 0, Loss: 1.000520\n",
      "Iteration 200, Loss: 0.000051\n",
      "Iteration 400, Loss: 0.000093\n",
      "Iteration 600, Loss: 0.000000\n",
      "Iteration 800, Loss: 0.000134\n",
      "Reconstructed: ##ᄉ that of of a a . in to 25\n",
      "Similarity: 0.1151\n",
      "Token alternatives:\n",
      "  Position 0: ##ᄉ | alternatives: タ(2.34), 1758(2.34)\n",
      "  Position 1: that | alternatives: a(1.79), at(1.79)\n",
      "  Position 2: of | alternatives: in(1.97), with(1.98)\n",
      "  Position 3: of | alternatives: in(1.86), to(1.86)\n",
      "  Position 4: a | alternatives: of(1.81), in(1.82)\n",
      "\n",
      "Trying with 15 tokens:\n",
      "Iteration 0, Loss: 0.993182\n",
      "Iteration 200, Loss: 0.000004\n",
      "Iteration 400, Loss: 0.000000\n",
      "Iteration 600, Loss: 0.000000\n",
      "Iteration 800, Loss: 0.000006\n",
      "Reconstructed: was in in in in to to in in in ##ת which of . a\n",
      "Similarity: 0.0552\n",
      "Token alternatives:\n",
      "  Position 0: was | alternatives: is(2.40), ##t(2.40)\n",
      "  Position 1: in | alternatives: of(1.83), to(1.83)\n",
      "  Position 2: in | alternatives: of(1.83), from(1.84)\n",
      "  Position 3: in | alternatives: on(1.73), of(1.76)\n",
      "  Position 4: in | alternatives: to(1.74), of(1.74)\n",
      "\n",
      "============================================================\n",
      "Original text: Quantum computers leverage superposition for parallel processing\n",
      "============================================================\n",
      "\n",
      "Trying with 5 tokens:\n",
      "Iteration 0, Loss: 0.985343\n",
      "Iteration 200, Loss: 0.000896\n",
      "Iteration 400, Loss: 0.000060\n",
      "Iteration 600, Loss: 0.000004\n",
      "Iteration 800, Loss: 0.000017\n",
      "Reconstructed: a a and a in\n",
      "Similarity: 0.0701\n",
      "Token alternatives:\n",
      "  Position 0: a | alternatives: ##ᆸ(2.18), an(2.18)\n",
      "  Position 1: a | alternatives: in(2.08), [MASK](2.09)\n",
      "  Position 2: and | alternatives: a(1.91), an(1.93)\n",
      "  Position 3: a | alternatives: the(2.01), in(2.02)\n",
      "  Position 4: in | alternatives: a(2.12), [unused738](2.12)\n",
      "\n",
      "Trying with 10 tokens:\n",
      "Iteration 0, Loss: 0.977105\n",
      "Iteration 200, Loss: 0.000077\n",
      "Iteration 400, Loss: 0.000002\n",
      "Iteration 600, Loss: 0.000000\n",
      "Iteration 800, Loss: 0.000000\n",
      "Reconstructed: ##য \" a a a a a a a the\n",
      "Similarity: -0.0176\n",
      "Token alternatives:\n",
      "  Position 0: ##য | alternatives: ##ᆸ(2.15), ᄊ(2.15)\n",
      "  Position 1: \" | alternatives: in(1.96), a(1.97)\n",
      "  Position 2: a | alternatives: ##ɒ(1.79), ,(1.79)\n",
      "  Position 3: a | alternatives: the(1.80), in(1.81)\n",
      "  Position 4: a | alternatives: the(1.67), an(1.72)\n",
      "\n",
      "Trying with 15 tokens:\n",
      "Iteration 0, Loss: 0.991968\n",
      "Iteration 200, Loss: 0.000043\n",
      "Iteration 400, Loss: 0.000000\n",
      "Iteration 600, Loss: 0.000000\n",
      "Iteration 800, Loss: 0.000000\n",
      "Reconstructed: ᄊ \" . , the the and and , the the へ a a in\n",
      "Similarity: 0.1515\n",
      "Token alternatives:\n",
      "  Position 0: ᄊ | alternatives: with(2.06), is(2.06)\n",
      "  Position 1: \" | alternatives: is(1.77), ,(1.77)\n",
      "  Position 2: . | alternatives: a(1.98), ん(1.98)\n",
      "  Position 3: , | alternatives: \"(1.92), a(1.92)\n",
      "  Position 4: the | alternatives: a(1.98), in(2.00)\n",
      "\n",
      "============================================================\n",
      "Original text: What causes the northern lights to appear in the sky?\n",
      "============================================================\n",
      "\n",
      "Trying with 5 tokens:\n",
      "Iteration 0, Loss: 0.928302\n",
      "Iteration 200, Loss: 0.001194\n",
      "Iteration 400, Loss: 0.000040\n",
      "Iteration 600, Loss: 0.000002\n",
      "Iteration 800, Loss: 0.000003\n",
      "Reconstructed: it is in to i\n",
      "Similarity: 0.1231\n",
      "Token alternatives:\n",
      "  Position 0: it | alternatives: in(1.94), is(1.95)\n",
      "  Position 1: is | alternatives: the(1.88), in(1.89)\n",
      "  Position 2: in | alternatives: to(2.01), ,(2.01)\n",
      "  Position 3: to | alternatives: is(2.43), the(2.43)\n",
      "  Position 4: i | alternatives: s(2.22), of(2.25)\n",
      "\n",
      "Trying with 10 tokens:\n",
      "Iteration 0, Loss: 0.933393\n",
      "Iteration 200, Loss: 0.000150\n",
      "Iteration 400, Loss: 0.000002\n",
      "Iteration 600, Loss: 0.000000\n",
      "Iteration 800, Loss: 0.000000\n",
      "Reconstructed: a it is of s ##lights in a the コ\n",
      "Similarity: 0.2766\n",
      "Token alternatives:\n",
      "  Position 0: a | alternatives: an(1.96), is(2.00)\n",
      "  Position 1: it | alternatives: to(1.82), is(1.83)\n",
      "  Position 2: is | alternatives: in(1.80), are(1.81)\n",
      "  Position 3: of | alternatives: in(1.72), to(1.73)\n",
      "  Position 4: s | alternatives: be(1.91), is(1.91)\n",
      "\n",
      "Trying with 15 tokens:\n",
      "Iteration 0, Loss: 0.944518\n",
      "Iteration 200, Loss: 0.000048\n",
      "Iteration 400, Loss: 0.000000\n",
      "Iteration 600, Loss: 0.000000\n",
      "Iteration 800, Loss: 0.000001\n",
      "Reconstructed: is a of the of the lights is a the the in of in northern\n",
      "Similarity: 0.4609\n",
      "Token alternatives:\n",
      "  Position 0: is | alternatives: will(2.13), are(2.14)\n",
      "  Position 1: a | alternatives: on(2.13), is(2.13)\n",
      "  Position 2: of | alternatives: an(2.09), a(2.09)\n",
      "  Position 3: the | alternatives: a(1.92), of(1.93)\n",
      "  Position 4: of | alternatives: shimmering(2.12), the(2.12)\n",
      "\n",
      "============================================================\n",
      "Original text: How does photosynthesis convert sunlight into energy?\n",
      "============================================================\n",
      "\n",
      "Trying with 5 tokens:\n",
      "Iteration 0, Loss: 0.989284\n",
      "Iteration 200, Loss: 0.001133\n",
      "Iteration 400, Loss: 0.000174\n",
      "Iteration 600, Loss: 0.000041\n",
      "Iteration 800, Loss: 0.000009\n",
      "Reconstructed: an the to and sunlight\n",
      "Similarity: 0.4882\n",
      "Token alternatives:\n",
      "  Position 0: an | alternatives: a(1.82), to(1.85)\n",
      "  Position 1: the | alternatives: [unused80](2.02), れ(2.03)\n",
      "  Position 2: to | alternatives: and(1.80), in(1.80)\n",
      "  Position 3: and | alternatives: the(1.89), to(1.89)\n",
      "  Position 4: sunlight | alternatives: photos(2.33), 光(2.37)\n",
      "\n",
      "Trying with 10 tokens:\n",
      "Iteration 0, Loss: 0.965660\n",
      "Iteration 200, Loss: 0.000653\n",
      "Iteration 400, Loss: 0.000070\n",
      "Iteration 600, Loss: 0.000008\n",
      "Iteration 800, Loss: 0.000001\n",
      "Reconstructed: to energy ##ツ to to to to to to sunlight\n",
      "Similarity: 0.4717\n",
      "Token alternatives:\n",
      "  Position 0: to | alternatives: does(2.24), do(2.27)\n",
      "  Position 1: energy | alternatives: sunlight(2.20), '(2.20)\n",
      "  Position 2: ##ツ | alternatives: sunlight(2.03), ##ハ(2.03)\n",
      "  Position 3: to | alternatives: in(1.89), [unused80](1.92)\n",
      "  Position 4: to | alternatives: in(1.88), on(1.91)\n",
      "\n",
      "Trying with 15 tokens:\n",
      "Iteration 0, Loss: 0.963373\n",
      "Iteration 200, Loss: 0.000273\n",
      "Iteration 400, Loss: 0.000013\n",
      "Iteration 600, Loss: 0.000000\n",
      "Iteration 800, Loss: 0.000000\n",
      "Reconstructed: to ##₂ ##₂ to to to to the the photos - to to and sunlight\n",
      "Similarity: 0.4438\n",
      "Token alternatives:\n",
      "  Position 0: to | alternatives: -(1.87), does(1.92)\n",
      "  Position 1: ##₂ | alternatives: 光(2.14), コ(2.14)\n",
      "  Position 2: ##₂ | alternatives: 風(1.99), h₂o(1.99)\n",
      "  Position 3: to | alternatives: '(1.98), of(1.98)\n",
      "  Position 4: to | alternatives: the(1.84), of(1.86)\n",
      "\n",
      "============================================================\n",
      "Original text: Why do some materials conduct electricity better than others?\n",
      "============================================================\n",
      "\n",
      "Trying with 5 tokens:\n",
      "Iteration 0, Loss: 0.966235\n",
      "Iteration 200, Loss: 0.001351\n",
      "Iteration 400, Loss: 0.000258\n",
      "Iteration 600, Loss: 0.000049\n",
      "Iteration 800, Loss: 0.000016\n",
      "Reconstructed: is [unused80] [unused80] to materials\n",
      "Similarity: 0.2642\n",
      "Token alternatives:\n",
      "  Position 0: is | alternatives: are(1.98), were(2.03)\n",
      "  Position 1: [unused80] | alternatives: in(2.16), and(2.19)\n",
      "  Position 2: [unused80] | alternatives: to(1.91), and(1.92)\n",
      "  Position 3: to | alternatives: of(1.98), [unused80](1.99)\n",
      "  Position 4: materials | alternatives: [unused80](2.22), ##त(2.23)\n",
      "\n",
      "Trying with 10 tokens:\n",
      "Iteration 0, Loss: 0.952161\n",
      "Iteration 200, Loss: 0.000819\n",
      "Iteration 400, Loss: 0.000094\n",
      "Iteration 600, Loss: 0.000013\n",
      "Iteration 800, Loss: 0.000002\n",
      "Reconstructed: [unused80] is is . . of ##ふ and to materials\n",
      "Similarity: 0.1952\n",
      "Token alternatives:\n",
      "  Position 0: [unused80] | alternatives: do(1.97), are(1.97)\n",
      "  Position 1: is | alternatives: h₂o(2.21), are(2.22)\n",
      "  Position 2: is | alternatives: in(1.88), are(1.88)\n",
      "  Position 3: . | alternatives: in(1.81), of(1.82)\n",
      "  Position 4: . | alternatives: and(1.78), ##☆(1.81)\n",
      "\n",
      "Trying with 15 tokens:\n",
      "Iteration 0, Loss: 0.949440\n",
      "Iteration 200, Loss: 0.000720\n",
      "Iteration 400, Loss: 0.000082\n",
      "Iteration 600, Loss: 0.000012\n",
      "Iteration 800, Loss: 0.000056\n",
      "Reconstructed: do my in and and to to [unused80] and and of of of of materials\n",
      "Similarity: 0.2907\n",
      "Token alternatives:\n",
      "  Position 0: do | alternatives: does(1.99), are(2.00)\n",
      "  Position 1: my | alternatives: s(2.28), h₂o(2.28)\n",
      "  Position 2: in | alternatives: is(1.90), of(1.91)\n",
      "  Position 3: and | alternatives: to(1.71), of(1.72)\n",
      "  Position 4: and | alternatives: ##₃(1.95), ##∈(1.96)\n",
      "\n",
      "============================================================\n",
      "Original text: Mix flour, eggs, and milk to create pancake batter\n",
      "============================================================\n",
      "\n",
      "Trying with 5 tokens:\n",
      "Iteration 0, Loss: 0.980613\n",
      "Iteration 200, Loss: 0.001836\n",
      "Iteration 400, Loss: 0.000303\n",
      "Iteration 600, Loss: 0.000050\n",
      "Iteration 800, Loss: 0.000007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_tokens \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m15\u001b[39m]:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrying with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     tokens, losses, final_embeds \u001b[38;5;241m=\u001b[39m \u001b[43minverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvert_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Reconstruct text\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_token\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens])\n",
      "Cell \u001b[0;32mIn[2], line 71\u001b[0m, in \u001b[0;36mEmbeddingInverter.invert_embedding\u001b[0;34m(self, target_embedding, num_tokens, num_iterations, lr)\u001b[0m\n\u001b[1;32m     68\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m F\u001b[38;5;241m.\u001b[39mcosine_similarity(predicted, target)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Update only the middle tokens (not CLS/SEP)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/py313/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py313/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py313/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 3. Run Inversion on Test Corpus\n",
    "\n",
    "# %%\n",
    "# Initialize inverter\n",
    "inverter = EmbeddingInverter(transformer_model, tokenizer, device)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Test on subset of corpus\n",
    "for i, (text, embedding) in enumerate(zip(test_corpus[:10], target_embeddings[:10])):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Original text: {text}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Try different sequence lengths\n",
    "    for num_tokens in [5, 10, 15]:\n",
    "        print(f\"\\nTrying with {num_tokens} tokens:\")\n",
    "        \n",
    "        tokens, losses, final_embeds = inverter.invert_embedding(\n",
    "            embedding, \n",
    "            num_tokens=num_tokens,\n",
    "            num_iterations=1000,\n",
    "            lr=0.01\n",
    "        )\n",
    "        \n",
    "        # Reconstruct text\n",
    "        reconstructed = ' '.join([t['best_token'] for t in tokens])\n",
    "        \n",
    "        # Verify by encoding reconstructed text\n",
    "        reconstructed_embedding = sentence_model.encode([reconstructed])[0]\n",
    "        similarity = cosine_similarity([embedding], [reconstructed_embedding])[0][0]\n",
    "        \n",
    "        print(f\"Reconstructed: {reconstructed}\")\n",
    "        print(f\"Similarity: {similarity:.4f}\")\n",
    "        \n",
    "        # Show alternatives\n",
    "        print(\"Token alternatives:\")\n",
    "        for j, token_info in enumerate(tokens[:5]):  # Show first 5\n",
    "            alts = ', '.join([f\"{t}({d:.2f})\" for t, d in token_info['alternatives'][:2]])\n",
    "            print(f\"  Position {j}: {token_info['best_token']} | alternatives: {alts}\")\n",
    "        \n",
    "        results.append({\n",
    "            'original': text,\n",
    "            'num_tokens': num_tokens,\n",
    "            'reconstructed': reconstructed,\n",
    "            'similarity': similarity,\n",
    "            'final_loss': losses[-1]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8dc486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Analyze Results\n",
    "\n",
    "# %%\n",
    "# Convert to DataFrame for analysis\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Group by sequence length\n",
    "print(\"\\nAverage similarity by sequence length:\")\n",
    "print(df_results.groupby('num_tokens')['similarity'].agg(['mean', 'std']))\n",
    "\n",
    "# Plot convergence for one example\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Optimization Loss Over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss (1 - cosine similarity)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf2d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Advanced: Multi-Start Optimization\n",
    "\n",
    "# %%\n",
    "def multi_start_inversion(inverter, target_embedding, num_starts=5, num_tokens=10):\n",
    "    \"\"\"\n",
    "    Run inversion from multiple random initializations\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for start in range(num_starts):\n",
    "        tokens, losses, _ = inverter.invert_embedding(\n",
    "            target_embedding,\n",
    "            num_tokens=num_tokens,\n",
    "            num_iterations=500,  # Fewer iterations per start\n",
    "            lr=0.01\n",
    "        )\n",
    "        \n",
    "        reconstructed = ' '.join([t['best_token'] for t in tokens])\n",
    "        reconstructed_embedding = sentence_model.encode([reconstructed])[0]\n",
    "        similarity = cosine_similarity([target_embedding], [reconstructed_embedding])[0][0]\n",
    "        \n",
    "        all_results.append({\n",
    "            'text': reconstructed,\n",
    "            'similarity': similarity,\n",
    "            'tokens': tokens\n",
    "        })\n",
    "    \n",
    "    # Sort by similarity\n",
    "    all_results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    return all_results\n",
    "\n",
    "# Test multi-start on one example\n",
    "test_text = test_corpus[4]  # \"Machine learning model achieves...\"\n",
    "test_embedding = target_embeddings[4]\n",
    "\n",
    "print(f\"Original: {test_text}\")\n",
    "print(\"\\nMultiple reconstruction attempts:\")\n",
    "\n",
    "results = multi_start_inversion(inverter, test_embedding, num_starts=5)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nAttempt {i+1}: {result['text']}\")\n",
    "    print(f\"Similarity: {result['similarity']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06157989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. Analysis: What Makes Inversion Easier/Harder?\n",
    "\n",
    "# %%\n",
    "# Test on different text types\n",
    "text_types = {\n",
    "    'short_factual': \"Water boils at 100 degrees Celsius\",\n",
    "    'long_technical': \"The convolutional neural network architecture uses multiple layers of learnable filters\",\n",
    "    'question': \"How does the immune system protect against pathogens?\",\n",
    "    'conversational': \"Hey, want to grab lunch tomorrow?\",\n",
    "    'unique_terms': \"Quantum entanglement enables instantaneous correlation\",\n",
    "    'common_words': \"The day was very nice and warm\"\n",
    "}\n",
    "\n",
    "print(\"Inversion difficulty by text type:\\n\")\n",
    "\n",
    "for text_type, text in text_types.items():\n",
    "    embedding = sentence_model.encode([text])[0]\n",
    "    \n",
    "    tokens, losses, _ = inverter.invert_embedding(embedding, num_tokens=10, num_iterations=500)\n",
    "    reconstructed = ' '.join([t['best_token'] for t in tokens])\n",
    "    \n",
    "    reconstructed_embedding = sentence_model.encode([reconstructed])[0]\n",
    "    similarity = cosine_similarity([embedding], [reconstructed_embedding])[0][0]\n",
    "    \n",
    "    print(f\"{text_type:20} | Similarity: {similarity:.4f}\")\n",
    "    print(f\"  Original:      {text}\")\n",
    "    print(f\"  Reconstructed: {reconstructed}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Conclusions\n",
    "\n",
    "# %%\n",
    "print(\"=== ITERATIVE OPTIMIZATION RESULTS ===\\n\")\n",
    "\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"1. Optimization successfully finds embeddings that produce high similarity\")\n",
    "print(\"2. Exact text recovery is rare, but semantic content is preserved\")\n",
    "print(\"3. Longer sequences generally achieve higher similarity\")\n",
    "print(\"4. Technical/unique terms are easier to recover than common phrases\")\n",
    "\n",
    "print(\"\\nLIMITATIONS:\")\n",
    "print(\"- Discrete token selection is challenging\")\n",
    "print(\"- Multiple valid reconstructions exist\")\n",
    "print(\"- Common words create ambiguity\")\n",
    "\n",
    "print(\"\\nSECURITY IMPLICATIONS:\")\n",
    "print(\"- Even without exact recovery, semantic content leaks\")\n",
    "print(\"- Unique/technical terms are especially vulnerable\")\n",
    "print(\"- Demonstrates embeddings are not safe for sensitive data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

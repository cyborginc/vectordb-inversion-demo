{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b7ac5d",
   "metadata": {},
   "source": [
    "# ChromaDB Plaintext Storage Demo\n",
    "\n",
    "This notebook demonstrates that ChromaDB stores embeddings in plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup ChromaDB\n",
    "\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import sqlite3\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create ChromaDB client with persistent storage\n",
    "persist_directory = \"./chromadb_demo\"\n",
    "client = chromadb.PersistentClient(path=persist_directory)\n",
    "\n",
    "# Create or get collection\n",
    "try:\n",
    "    collection = client.create_collection(\"sensitive_documents\")\n",
    "except:\n",
    "    client.delete_collection(\"sensitive_documents\")\n",
    "    collection = client.create_collection(\"sensitive_documents\")\n",
    "\n",
    "print(f\"ChromaDB storage location: {persist_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Add Sensitive Documents\n",
    "\n",
    "# Sample sensitive documents\n",
    "documents = [\n",
    "    \"Patient John Doe, SSN 123-45-6789, diagnosed with diabetes\",\n",
    "    \"Credit card number 4532-1234-5678-9012 belongs to Jane Smith\",\n",
    "    \"API key: sk-1234567890abcdef, expires 2024-12-31\",\n",
    "    \"Database password: MyS3cr3tP@ssw0rd! for production server\",\n",
    "    \"Employee ID 12345 salary: $120,000 annual compensation\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(documents)\n",
    "\n",
    "# Add to ChromaDB\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    embeddings=embeddings.tolist(),\n",
    "    ids=[f\"doc_{i}\" for i in range(len(documents))],\n",
    "    metadatas=[{\"sensitive\": True, \"doc_num\": i} for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "print(f\"Added {len(documents)} sensitive documents to ChromaDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52982fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run a Normal Query on ChromaDB\n",
    "\n",
    "# Normal query\n",
    "query = \"financial information\"\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding.tolist(),\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(\"Query results:\")\n",
    "for i, doc in enumerate(results['documents'][0]):\n",
    "    print(f\"{i+1}. {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd043dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dump ChromaDB Storage - SQLite Database\n",
    "\n",
    "# ChromaDB uses SQLite for metadata and Parquet for embeddings\n",
    "# Let's examine both\n",
    "\n",
    "# First, let's look at the SQLite database\n",
    "db_path = os.path.join(persist_directory, \"chroma.sqlite3\")\n",
    "print(f\"SQLite database path: {db_path}\")\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# List all tables\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"\\nTables in ChromaDB SQLite:\")\n",
    "for table in tables:\n",
    "    print(f\"  - {table[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce781dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5. Examine Raw Embedding Storage\n",
    "\n",
    "# # ChromaDB stores embeddings in Parquet files\n",
    "# # Let's find and examine them\n",
    "\n",
    "# import pyarrow.parquet as pq\n",
    "# import glob\n",
    "\n",
    "# # Find parquet files\n",
    "# parquet_files = glob.glob(os.path.join(persist_directory, \"**/*.parquet\"), recursive=True)\n",
    "# print(f\"Found {len(parquet_files)} Parquet files\")\n",
    "\n",
    "# if parquet_files:\n",
    "#     # Read the first parquet file\n",
    "#     parquet_path = parquet_files[0]\n",
    "#     print(f\"\\nReading: {parquet_path}\")\n",
    "    \n",
    "#     # Read parquet file\n",
    "#     table = pq.read_table(parquet_path)\n",
    "#     df = table.to_pandas()\n",
    "    \n",
    "#     print(f\"\\nParquet file shape: {df.shape}\")\n",
    "#     print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "#     # Display first few rows\n",
    "#     print(\"\\nFirst few rows of embeddings:\")\n",
    "#     print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7eb917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Direct Memory Inspection\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Force a query to ensure embeddings are loaded in memory\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding.tolist(),\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "# Look for numpy arrays in memory\n",
    "print(\"Searching for embedding arrays in memory...\")\n",
    "embedding_arrays = []\n",
    "for obj in gc.get_objects():\n",
    "    if isinstance(obj, np.ndarray) and obj.shape == (384,):  # all-MiniLM-L6-v2 produces 384-dim embeddings\n",
    "        embedding_arrays.append(obj)\n",
    "\n",
    "print(f\"Found {len(embedding_arrays)} potential embedding vectors in memory\")\n",
    "\n",
    "if embedding_arrays:\n",
    "    print(\"\\nSample embedding vector (first 10 dimensions):\")\n",
    "    print(embedding_arrays[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e429528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Demonstrate Plaintext Storage\n",
    "\n",
    "# Let's read the raw data and show it's in plaintext\n",
    "print(\"=== PLAINTEXT STORAGE DEMONSTRATION ===\\n\")\n",
    "\n",
    "# 1. Documents are stored in plaintext\n",
    "cursor.execute(\"SELECT * FROM embeddings_queue\")\n",
    "queue_data = cursor.fetchall()\n",
    "if queue_data:\n",
    "    print(\"Documents in embeddings_queue:\")\n",
    "    for row in queue_data:\n",
    "        print(row)\n",
    "\n",
    "# 2. Embeddings are stored as plain floating point numbers\n",
    "if parquet_files:\n",
    "    print(\"\\n\\nRaw embedding values from Parquet file:\")\n",
    "    # Get embedding column\n",
    "    if 'embedding' in df.columns:\n",
    "        first_embedding = df['embedding'].iloc[0]\n",
    "        if isinstance(first_embedding, np.ndarray):\n",
    "            print(f\"First embedding vector (shape: {first_embedding.shape}):\")\n",
    "            print(f\"First 20 values: {first_embedding[:20]}\")\n",
    "            print(f\"Min: {first_embedding.min():.6f}, Max: {first_embedding.max():.6f}\")\n",
    "            print(f\"Mean: {first_embedding.mean():.6f}, Std: {first_embedding.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "# First, check the column names\n",
    "cursor.execute(\"PRAGMA table_info(embeddings_queue)\")\n",
    "columns = cursor.fetchall()\n",
    "print(\"Embeddings_queue columns:\")\n",
    "for col in columns:\n",
    "    print(f\"  {col[1]} ({col[2]})\")\n",
    "\n",
    "# Get all data from embeddings_queue\n",
    "cursor.execute(\"SELECT * FROM embeddings_queue LIMIT 5\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "print(f\"\\nFound {len(rows)} embedding records\")\n",
    "\n",
    "# Based on your data, columns appear to be:\n",
    "# id, timestamp, seq_num, collection_id, doc_id, embedding_blob, data_type, metadata\n",
    "for i, row in enumerate(rows):\n",
    "    # Extract fields based on the actual structure\n",
    "    queue_id = row[0]\n",
    "    timestamp = row[1]\n",
    "    doc_id = row[4]  # e.g., 'doc_0', 'doc_1'\n",
    "    embedding_blob = row[5]  # The binary embedding data\n",
    "    data_type = row[6]  # Should be 'FLOAT32'\n",
    "    metadata = row[7]  # JSON metadata\n",
    "    \n",
    "    print(f\"\\n--- Document {i} (ID: {doc_id}) ---\")\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    print(f\"Metadata: {metadata[:100]}...\")  # First 100 chars of metadata\n",
    "    \n",
    "    # Parse FLOAT32 binary data\n",
    "    # Each float32 is 4 bytes\n",
    "    num_floats = len(embedding_blob) // 4\n",
    "    \n",
    "    # Unpack the binary data as float32 values\n",
    "    embedding_values = struct.unpack(f'{num_floats}f', embedding_blob)\n",
    "    \n",
    "    print(f\"Embedding dimension: {num_floats}\")\n",
    "    print(f\"First 20 embedding values:\")\n",
    "    for j in range(min(20, len(embedding_values))):\n",
    "        print(f\"  [{j}]: {embedding_values[j]:.6f}\")\n",
    "    \n",
    "    # Show statistics\n",
    "    embedding_array = np.array(embedding_values)\n",
    "    print(f\"\\nEmbedding statistics:\")\n",
    "    print(f\"  Min: {embedding_array.min():.6f}\")\n",
    "    print(f\"  Max: {embedding_array.max():.6f}\")\n",
    "    print(f\"  Mean: {embedding_array.mean():.6f}\")\n",
    "    print(f\"  Std: {embedding_array.std():.6f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Demonstrate Direct Access to Sensitive Data\n",
    "\n",
    "# %%\n",
    "print(\"\\n=== DIRECT ACCESS TO SENSITIVE DATA ===\\n\")\n",
    "\n",
    "# Extract and display the actual sensitive documents\n",
    "cursor.execute(\"SELECT * FROM embeddings_queue\")\n",
    "all_rows = cursor.fetchall()\n",
    "\n",
    "print(\"All documents stored in PLAINTEXT:\")\n",
    "for row in all_rows:\n",
    "    metadata = row[7]  # Metadata is in the 8th column\n",
    "    # Parse the JSON metadata\n",
    "    import json\n",
    "    meta_dict = json.loads(metadata)\n",
    "    if 'chroma:document' in meta_dict:\n",
    "        print(f\"\\n• {meta_dict['chroma:document']}\")\n",
    "\n",
    "print(\"\\n⚠️  Anyone with database access can read all sensitive information!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Cleanup\n",
    "\n",
    "# Cleanup connection\n",
    "# conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
